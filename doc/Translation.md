# 高分辨率多视点立体视觉与立体匹配的级联代价体

* [摘要](#摘要)
* [1 背景介绍](#1-背景介绍)
* [2 相关工作](#2-相关工作)
* [3 本文方法](#3-本文方法)
   * [3.1 代价体公式](#31-代价体公式)
   * [3.2 级联代价体](#32-级联代价体)
   * [3.3 特征金字塔](#33-特征金字塔)
   * [3.4 损失函数](#34-损失函数)
* [4 实验](#4-实验)
   * [4.1 多视点立体视觉](#41-多视点立体视觉)
   * [4.2 立体匹配](#42-立体匹配)
   * [4.3 消融实验](#43-消融实验)
   * [4.4 运行时间和显存空间](#44-运行时间和显存空间)
* [5 结论](#5-结论)
* [6 附录](#6-附录)
   * [6.1 讨论](#61-讨论)
   * [6.2 多视点立体视觉](#62-多视点立体视觉)
   * [6.3 立体匹配](#63-立体匹配)
   * [6.4 限制和未来的工作](#64-限制和未来的工作)

------

## 摘要

深度多视点立体视觉（multi-view stereo，简称MVS）和立体匹配问题经常构建3D代价体，用以正则化和回归深度和视差。这些方法在高分辨率输出时，随着体积分辨率的增加，显存和时间成本呈立方增加。本文基于现有依靠3D代价体进行多视点立体视觉和立体匹配的方法，提出了一种显存和时间高效的代价体表示。首先，本文提出的代价体通过编码几何信息和上下文并逐步细化的特征金字塔进行构建。接着，我们可以通过前一阶段的预测缩小每个阶段的深度（或视差）范围。随着更高的代价体分辨率和深度（或视差）间隔的自适应调整，输出将不断由粗变细进行更好的恢复。

我们将级联代价体应用到有代表性的MVS-Net中，获得了DTU数据集上35.6%的提升（第一名），同时减少50.6%的GPU消耗和59.3%的运行时间消耗。该方法也在Tanks and Temples排行榜上，在所有深度模型中排名第一。在其他具有代表性的立体视觉CNNs网络上的准确率、运行时间和GPU消耗统计，同样验证了本文方法的有效性。本文的代码开源在：https: //github.com/alibaba/cascade-stereo



------

## 1 背景介绍

卷积神经网络（Convolutional neural networks，简称CNNs）已经被广泛的应用在三维重建和更广泛的计算机视觉任务中。最先进的多视点立体视觉[19,29,52,53]和立体匹配算法[3,15,22,33,26,56]通常根据一组假设的深度（或视差）和投影特征计算3D代价体。并通过3D卷积将代价体进行正则化和回归，得到最终的场景深度（或视差）。

与2D CNNs方法[30,55]进行对比，3D代价体能更好的提取几何结构并在三维空间中进行光度匹配，同时可以缓解透视变换和遮挡对图像失真的影响[4]。然而，由于3D CNNs的高时间和GPU消耗，这些构建3D代价体的方法通常受限于低分辨率的输入图像（和结果）。通常，这些方法对特征图进行下采样，以形成较低分辨率下的代价体[3,4,15,19,22,29,33,46,52,53,56]，并且采用上采样[3,15,22,33,42,46,49,56]或者后处理[4,29]输出最终高分辨率的结果。

受之前由粗到细（coarse-to-fine）的深度学习立体视觉方法启发[8,9,11]，我们提出了新的3D代价体级联表达。首先通过一个特征金字塔提取标准多视点立体视觉[52]和立体匹配网络[3,15]中的多尺度特征。在由粗到细的情况中，早期阶段的代价体是基于更大规模的语义2D特征和稀疏采样的深度假设，这导致了相对较低的体积分辨率。随后使用早期阶段估计出的深度（或视差）图，自适应地调整深度（或视差）假设的采样范围，并应用更精细的语义特征构建新的代价体。这种自适应深度采样和特征分辨率的调整确保了计算和显存资源花在更有益的地方。通过这种方式，我们的级联结构可以显著地降低计算时间和GPU消耗。可以从图1中观察本文方法的高效。

> 图1 最先进的深度学习多视点立体视觉方法和MVSNet+Ours的比较。(a)-(d): MVSNet, R-MVSNet, Point-MVSNet和MVSNet+Ours的重建点云结果； (e)、 (f): 重建精度与GPU消耗和运行时间的关系。输入图像分辨率为$1152 \times 864$。

我们在多视点立体视觉和立体匹配的很多不同基准数据集上验证了我们的方法。多视点立体视觉方面，我们的级联结构融合MVSNet[52]在本文提交的时间上获得DTU数据集[1]上最好的表现。同时也是Tanks and Temples[24]基准测试上最先进的基于深度学习的方法。立体匹配方面，我们的方法使GwcNet[15]的终点误差和GPU消耗分别降低15.2%和36.9%。



------

## 2 相关工作

A. 立体匹配

根据Scharstein等人[38]的调研，典型的立体匹配肃反啊包含四个步骤：匹配代价计算，匹配代价累计，视差计算和视差优化。一些方法[31,50,57]对临近像素的匹配代价进行聚合，通常采用赢家通吃策略选择最优视差。通用方法构[17,23,43]建能量函数并尝试最小化其以寻找最优视差。更特殊的一些方法[23,43]使用信念传播和半全局匹配[17]，用动态规划逼近全局最优。

在深度神经网络的上下文中，基于CNNs的立体匹配方法首先由Zbontar和LeCun[54]提出，在他们的方法中，引入了卷积神经网络学习小图像块对的相似性度量。广泛应用的3D代价体首先由GCNet[22]提出，其中视差回归步骤使用soft argmin操作选择最佳匹配结果。PSMNet[3]进一步提出金字塔空间池化和3D沙漏网络，对于代价体正则化产生了更好的结果。GwcNet[15]调整了3D沙漏的结构并引入分组关联，形成了基于3D代价体的分组。HSM[48]构建了有层次设计的高分辨率图像的轻模型。EMCUA[33]提出多层级上下文的超聚合。GANet[56]构建多个半全局聚合层和有指导的聚合层进一步提升准确度。Deep-Pruner[5]是由粗到细的方法，通过可微PatchMatch模块对每个像素预测搜索范围。

虽然基于3D代价体的方法显著地提升了性能，但它们受限于下采样代价体，并且依赖插值操作生成高分辨率的视差。我们的级联代价体可以融合这些方法提升准确度并降低GPU消耗。



B. 多视点立体视觉

根据全面的调研[12]，传统多视点立体匹配方法可以大致分为：估计每个像素和表面关系的体素方法[20,21,25,41]，直接进行三维点迭代进行密集结果生成的点云方法[13,26]和使用一张参考图像和几张原始图像估计单张深度图的深度图估计方法[2,7,14,40,44,51]。对于大规模的从运动中恢复结构（Structure-from-Motion，简称SfM），一些方法[58,59]使用基于分布式运动平均和全局摄影机一致的分布式方法。

近期，基于深度学习的方法在多视点立体视觉上也表现出了优越的性能。多图像块相似性[16]提出了学习成本指标。SurfaceNet[20]和DeepMVS[18]预先将多视点头像投影到三维空间并且使用深度网络进行正则化和聚合。最近，使用3D代价体的多视点立体视觉方法不断提出[4,6,10,19,29,52,53]。3D代价体通过将多视点图像的2D图像特征进行投影，接着使用3D CNNs进行代价正则化和深度回归。由于3D CNNs需要大量GPU梓源，这些方法通常将代价体进行下采样。我们的级联代价体可以轻松的集成进这些方法进行高分辨率的代价体构建，并且提高准确度，提高计算速度，降低GPU消耗。



C. 立体视觉和MVS中的高分辨率输出

近期，一些基于深度学习的方法尽量减少显存需求，以产生高分辨率的输出。相较于采用体素网格，Point MVSNet[4]提出使用小代价体生成粗深度，再采用基于点的迭代细化网络以输出全分辨率。相比之下，结合了我们的级联代价体的标准MVS-Net可以输出圈分辨率的深度图，且准确性更高、时间和GPU消耗比Point MVSNet更低。一些工作[35,45]将高级空间进行划分以减少显存消耗，但这种固定代价体的表示缺乏灵活度。一些工作[29,42,49]通过2D CNNs构建额外的优化模型，输出高分辨率的预测。值得注意的是，这些优化模型可以与我们提出的级联代价体进行联合使用。



------

## 3 本文方法

本章对现有基于3D代价体的多视点立体视觉和立体方法进行补充，讲述级联代价体的架构细节。我们使用典型的MVSNet[52]和PSMNet[3]分别作为骨干网络展示级联代价体在多视点立体视觉和立体匹配任务中的应用。图2描述MVSNet融合我们方法的架构图。

> 图2 本文提出的级联代价体在MVSNet上的网络架构，记为MVSNet+Ours

### 3.1 代价体公式

基于深度学习的多视点立体视觉[4,52,53]和立体匹配方法[3,15,22,54,56]构建3D代价体用以度量相关图像对之间的相似性，并据此决定哪些可以匹配。在多视点立体视觉和立体匹配问题中，构建3D代价体都需要三个主要的步骤

（1）确定离散的假设深度（或视差）平面

（2）将每个视点将提取出的2D特征图投影到假设平面用以构建代价体，最后融合生成3D代价体

（3）像素级成本计算通常是模糊的固有不确定区域，如遮挡区域、遮挡模式、弱纹理区域和反射表面。为解决这一问题，通常引入多尺度的3D CNNs聚合上下文信息，对噪声污染的代价体进行正则化。

A. 多视点立体视觉的3D代价体

MVSNet提出在不同深度下使用相机前平行投影平面作为假设平面，并且深度范围通常由稀疏重建决定。坐标映射由单应变换决定：
$$
H_{i}(d)=K_{i} \cdot R_{i} \cdot\left(I-\frac{\left(t_{1}-t_{i}\right) \cdot n_{1}^{T}}{d}\right) \cdot R_{1}^{T} \cdot K_{1}^{-1}
$$
其中$H_i(d)$代表第$i$个视点和参考特征图在深度为$d$下的单应变换，$K_i, \ R_i, \ t_i$代表相机内参，分别是第$i$个视点相机的旋转和平移，$n_1$为基准相机的主轴。随后可微单应投影变换用于将2D特征图投影到参考图像的假设平面以组成特征体。为了将多个特征体聚合成一个代价体，MVSNet提出基于方差的代价度量方法以使用任意数量的输入特征体。

B. 立体匹配的3D代价体

PSMNet[3]使用视差水平作为假设平面，根据具体场景设计视差范围。由于左右图像已经被矫正，坐标的映射由$x$轴方向的偏移量决定
$$
C_r(d) = X_l - d
$$
其中$C_r(d)$代表右视图在视差$d$处经变换得到的$x$轴坐标，$X_l$是原始做图像的$x$轴坐标。为了构建特征体，我们使用沿x轴的平移将右视图的特征图投影到左视图中。有多种方法可以构建最终的代价体。GCNet[22]和PSMNet[3]在不降低特征维数的情况下连接左右特征体。其他工作[55]使用绝对差的和计算匹配代价。DispNetC[30]计算左特征体和右特征体的全相关性并且对每个视差等级产出唯一的单通道相关图。GwcNet[15]通过将特征划分为组并在每个组中计算相关映射来进行分组相关。

### 3.2 级联代价体

> 图3 左：标准代价体，$D$为假设平面数量，$W \times H$为空间分辨率，$I$为平面间隔；右：效率（运行时间与GPU消耗）和准确性的影响因素

图3展示维度为$W \times H \times D \times F$的标准特征体，其中$W \times H$代表空间分辨率，$D$是深度平面假设，$F$是特征图的通道数。在其他工作[4,52,53]中提到的，深度评价假设数$D$和空间分辨率$W \times H$的增加，以及更精细平面的间隔可以提高重建精度。然而，GPU消耗和运行时间会随着代价体分辨率的增加而立方增加。R-MVSNet和MVSNet演示可以在16GB的Tesla P100 GPU上处理最高$H \times W \times D \times F = 1600 \times 1184 \times 256 \times 32$维度的代价体。为了解决上述问题，我们提出一种级联代价体表达并且通过由粗到细的方法预测输出。

A. 假设范围

如图4所示，第一阶段的深度（或视差）范围由$R_1$表示，涵盖了输入场景的整个深度（或视差）范围。在接下来的阶段，可以在前一阶段的预测输出基础上，缩小假设范围。因此，我们有$R_{k+1}=R_k \cdot w_k$，其中$R_k$代表第$k$步的假设范围，$W_k < 1$是假设范围的缩小因子。

> 图4 假设平面生成的说明。$R_k,I_k$分别是第$k$阶段的假设范围和假设平面数量。粉线代表假设平面，黄线代表第一阶段的预测深度（或视差）图，用于决定第二阶段的假设范围和假设平面间隔。

B. 假设平面间隔

用$I_1$表示第一阶段的深度（或视差）间隔。相比于一般采用的单代价体[3,52]表达，初始假设平面间隔相对较大，以产生粗深度（或视差）估计。在接下来的阶段，应用更精细的假设平面间隔恢复更精细的输出。因此$I_{k+1} = I_k \cdot p_k$，其中$I_k$是第$k$步假设平面间隔，$p_k < 1$是假设平面间隔的缩小因子。

C. 假设平面数量

在第$k$步，假设范围为$R_k$，假设平面间隔为$I_k$，假设平面数量$D_k$由公式$D_k = R_k / I_k$计算。当代价体的空间分辨率固定后，更大的$D_k$生成更多的假设平面并能得到更精确的结果，但同时也导致更高的GPU消耗和运行时间。根据级联构建，由于假设范围逐步显著缩小，但仍然覆盖整个输出范围，我们可以高效的减少假设平面的总数。

D. 空间分辨率 

根据特征金字塔网络[28]，我们将每一步骤的代价体空间分辨率进行翻倍，同时将原始特征图的分辨率翻倍。我们定义$N$作为级联特征体的总阶段步骤，则第$k$阶段的代价体空间分辨率定义为$\frac{W}{2^{N-k}} \times \frac{H}{2^{N-k}}$。在多视点立体视觉任务中设定$N=3$，在立体匹配任务重设定$N=2$。

E. 投影操作

将级联代价体构建应用到多视点立体视觉中，我们在公式1中使用不同的单应变换，第$k+1$阶段公式如下：
$$
H_{i}\left(d_{k}^{m}+\Delta_{k+1}^{m}\right)=K_{i} \cdot R_{i} \cdot\left(I-\frac{\left(t_{1}-t_{i}\right) \cdot n_{1}^{T}}{d_{k}^{m}+\Delta_{k+1}^{m}}\right) \cdot R_{1}^{T} \cdot K_{1}^{-1}
$$
其中$d_k^m$表示第$k$阶段中第$m$个像素估计出的深度，$\Delta^m_{k+1}$是第$k+1$个阶段中第$m$个像素要学习的剩余深度。

与立体匹配相似，我们使用级联代价体重新改写公式2。第$k+1$个阶段中第$m$个像素的坐标投影为：
$$
C_{r}\left(d_{k}^{m}+\Delta_{k+1}^{m}\right)=X_{l}-\left(d_{k}^{m}+\Delta_{k+1}^{m}\right)
$$
其中$d_k^m$表示第$k$阶段中第$m$个像素估计出的视差，$\Delta^m_{k+1}$是第$k+1$个阶段中第$m$个像素要学习的剩余视差。

### 3.3 特征金字塔

为了获取高分辨率的深度（或视差）图，之前的工作[29,33,46,56]通常使用标准代价体生成一个相对低分辨率的深度（或视差）图，再上采样或使用2D CNNs进行完善。标准代价体使用最顶级特征映射构建的，它包含高级语义特征，但缺乏低级精细表示。本文我们参考特征金字塔网络[28]，并且采用其增加空间分辨率的特征图，构建更高分辨率的代价体。比如，在MVSNet应用级联代价体时，我们从特征金字塔网络的特征映射$\{P_1, P_2, P_3\}$构建三个代价体，对应的空间分辨率为输入图像尺寸的$\{1/16, 1/4, 1 \}$。

### 3.4 损失函数

有$N$个阶段的级联代价体产生$N-1$个中间输出和最终预测。我们对所有输出进行监督，总损失定义为
$$
\text { Loss }=\sum_{k=1}^{N} \lambda^{k} \cdot L^{k}
$$
其中$L^k$代表第$k$阶段的损失，$\lambda^k$表示它对应的损失权重。在我们的实验中采取和骨干网一样的损失函数$L^k$。

------

## 4 实验

我们在多视点立体视觉和立体匹配任务中评估提出的级联代价体。

### 4.1 多视点立体视觉

A. 数据集

DTU[1]是一个由124哥不同场景组成的大规模MVS数据集，在7种不同的光照条件下，选取49或64个位置进行拍摄。Tanks and Temples[24]数据集包含具有小深度范围的真实场景。具体来说，中间集由8个场景组成，包括家庭、弗朗西斯、马、灯塔、M60、豹、操场和火车。与标准方法[53]相同，我们使用DTU训练集训练我们的方法，并且在DTU评价集中进行测试。我们同样用DTU数据集上训练的模型，在Tanks and Temples的中间集中验证本文方法的泛化性，没有经过任何微调。

B. 实现

我们将提出的级联代价体应用于具有代表性的MVSNet中，并将该网络表示为MVSNet+Ours。在训练过程中，我们设置输入图像数量$N=3$，图像分辨率为$640 \times 512$。在平衡精度和效率后，我们采用三级级联代价体。从第一阶段到第三阶段，深度假设数分别为48，32和8，对应的深度间隔分别为MVSNet间隔的4，2和1倍。相应的，特征图的空间分辨率增加到原始输入图像的1/16，1/14和1倍。我们在训练和评估中与MVSNet采取同样的输入视点选择和数据预处理策略。在训练过程中，我们使用参数为$\beta_1 = 0.9, \ \beta_2 = 0.999$的Adam优化器。初始学习率设为0.001，经过16轮训练后训练完成，其中在10，12和14轮之后下采样到原来的2倍。我们使用8个NVIDIA GTX 1080 Ti GPU训练我们的方法，每个GPU上有两个训练样本。
针对DTU数据集上的定量评估，我们使用DTU数据集提供的MATLAB代码计算准确度和完整度。在MVSNet的基础上实现了百分比计算。F-score作为Tanks and Temples数据集的评价指标，衡量重建点云的准确性和完整性。我们使用fusibile[36]作为后处理的工具，主要包含三个步骤：光度一致性滤波、几何一致性滤波和深度融合。

C. 基准测试表现

表格1展示了在DTU评估集上的定量结果。融合级联代价体的MVSNet方法在完整性和整体质量上比其他方法更好[4,29,52,53]，并在DTU数据集上排名第一，有着35.6%的提升，50.6%的显存消耗减少和59.3%的运行时间消耗减少。定性结果如图5所示。我们可以看到MVSNet+Ours在基于深度学习的多视点立体视觉问题中达到了最先进的性能。Tanks and Temples中间集的定量点云结果如图6所示。值得注意的是，除了R-MVSNet[53]之外，我们通过运行这些方法提供的预训练模型和代码来获得上述方法的结果，而R-MVSNet[53]使用它们的后处理方法提供点云结果。

为了分析每个阶段的准确度、GPU消耗和运行时间，我们在DTU数据集上评估MVSNet+Ours。我们在表格9中提供全面的统计数据，并在图7中展现可视化结果。在由粗到细的情形下，整体质量从0.602提升至0.355。相应的，GPU消耗从2373MB到4093MB和5345MB，运行时间从0.081s提升到0.243s和0.492s。

> 表1 DTU数据集上不同方法的多视角立体视觉结果(越低越好)。我们使用PointMVSNet的两种分辨率设置来进行这个实验，其中MVSNet+Ours的分辨率为$1152 \times 864$。
>
> 表2 Tanks and Temples数据集上最先进方法和我们方法的统计结果
>
> 表3 不同阶段级联代价体的统计结果。统计数据使用MVSNet+Ours在DTU计算集上收集得到。运行时是当前阶段和之前阶段的总和。本实验输入图像的分辨率为$1152 \times 864$。

> 图5 DTU数据集上场景10的多视点立体视觉定性结果。第一行：不同方法生成的点云和真值点云；第二行：放缩的局部区域。

> 图6 MVSNet+Ours在Tanks and Temples中间集上重建的点云结果

> 图7 每一阶段重建的结果。第一行：深度图真值和中间重建；第二行：中间重建的误差图

### 4.2 立体匹配

A. 数据集

Scene Flow数据集[30]是包含35454训练立体视觉对和4370测试立体视觉对的大型数据集，立体视觉对尺寸为$960 \times 540$，包含了精确的视差图真值。我们使用Scene Flow流数据集的Finalpass，因为包含了更多的运动模糊和散焦，与真实场景更为相近。KITTI 2015[32]是具有动态街景的真实视觉数据集，包含200个训练图像对和200个测试图像对。Middlebury[37]是公开可用的高分辨率立体匹配数据集，包含60对不完善的校准、不同的曝光和不同的光照条件。

B. 实现

在Scene Flow数据集中，我们用本文提出的级联代价体扩充PSMNet、GwcNet和GANet11[56]，并将它们表示为PSMNet+Ours, GwcNet+Ours and GANet11+Ours。在准确性和效率的平衡下，我们使用两阶段的级联代价体，视差平面数设为12。相应的视差间隔分别设为4和1。特征图的空间分辨率设为原始输入图像的1/16和1/4。最大的视差为192。

在KITTI 2015基准测试上，我们主要比较GwcNet与GwcNet+Ours。为了进行公平的比较，我们遵循原始网络的训练细节。Scene Flow数据集的评价指标选取终点误差（end-point-error，简称EPE），它逐像素计算绝对视差误差。对于KITTI 2015数据集，选择视差离群值$D_1$评估视差误差大于$max(3px,0.005d^*)$的像素，其中$d^*$代表视差真值。

C. 基准测试表现

Scene Flow数据集上不同立体匹配方法的定量结果如表4所示。通过应用级联3D代价体，我们提高了所有指标的准确性和更少的显存需求。我们的方法减少EPE从0.166到0.116和0.050，PSMNet（0.887 vs. 0.721）, GwcNet（0.765 vs. 0.649） ，GANet11 [56] （0.950 vs. 0.900）。在大于1px上有明显的提高，表明引入高分辨率代价体可以显著地抑制小误差。在KITTI 2015数据集上，表格5展现了背景、前景和所有像素的视差离群值$D_1$的百分比。与原始GwcNNet相比，GwcNet+Ours排名从第29到第27（2019年11月5日）。图8展示了一些KITTI 2015测试集上的视差估计结果。在Middlebury数据集中，PSMNet+Ours排名第37位(2020年2月7日)。

> 表4 Scene Flow数据集上融合级联代价体和不融合方法的定量结果。准确度、GPU消耗和时间消耗被进行比较。
>
> 表5 不同立体匹配方法在KITTI 2015基准测试上的比较结果
>
> 图8 KITTI 2015数据集上定性的结果。第一行：输入图像；第二行：PSMNet的结果；第三行：GwcNet的结果；最后一行：融合级联代价体的GwcNet结果（GwcNet+Ours）。

### 4.3 消融实验

广泛的消融研究验证了我们方法提高的准确性和效率。除另有说明外，所有结果均由DTU验证集上的三阶段模型获得。

A. 级联级数

不同阶段数的定量结果见表6。在我们的实现中，我们使用192个深度假设的MVSNet作为基线模型，并用我们的级联设计代替它的代价体积，它同样包含192个深度假设。需要注意的是，不同阶段的空间分辨率与原始的MVSNet是相同的。扩展的MVSNet方法记为MVSNET-Cas$_i$，其中$i$代表总阶段数。我们发现，随着阶段数的增加，整体质量先显著增加后趋于稳定。

> 表6 MVSNet与融合级联代价体的MVSNet方法在不同深度假设数和深度间隔的对比。统计结果在DTU数据集上采集。

B. 空间分辨率

然后，我们研究了$W \times H$维代价体的空间分辨率对重建性能的影响。我们比较MVSNET-Cas$_3$，其中包含三个阶段，并且所有阶段共享相同的空间分辨率。MVSNET-Cas$_3$-Ups中所有原始图像尺寸空间分辨率从1/16提升到1，并且采用双线性插值方法对特征图进行上采样。如表7所示，MVSNet+Ours的整体质量显著优于MVSNET-Cas$_3$（0.453 vs. 0.355）。相应的，更高的空间分辨率带来GPU消耗增加（2373 vs. 5345 MB）和运行时间的增加 （0.322 vs. 0.492 seconds）。

> 表7 MVSNet与不同级联代价体设置的MVSNet方法的定量比较。具体说明，cascade代表原代价体分为三个级联代价体；up sample代表通过双线性上采样对相应的特征图增加空间分辨率的成本量；feature pyramid代表建立在金字塔特征图上具有更高空间分辨率的成本量。统计结果在DTU数据集上采集。

C. 特征金字塔

如表格7所示，代价体由特征金字塔网络构建，记为MVSNet+Ours可以稍微讲整体质量由0.379提升至0.355。GPU消耗（6227 vs. 5345 MB）和运行时间（0.676 vs. 0.492 seconds）也下降。与MVSNET-Cas$_3$和MVSNET-Cas$_3$-Ups相比，空间分辨率的提高对重建精度的提高更为关键。

D. 代价体正则化中的参数共享

我们还分析了所有阶段的权重共享对3D代价体正则化的影响。如表6所示，共享参数的级联代价体由MVSNET-Cas$_3$-share表示，比MVSNET-Cas$_3$中表现更差。结果表明，对不同阶段的级联代价体进行单独的参数学习，可以进一步提高精度。

### 4.4 运行时间和显存空间

表格1展现了考虑级联代价体和不考虑级联代价体的MVSNet关于GPU消耗和运行时间的比较。由于精度的显著提高，GPU显存从10823MB下降到5345MB，运行时间从1.210秒下降到0.492秒。在表格4中展现了考虑级联代价体和不考虑级联代价体的PSMNet、GwcNet和GANet11关于GPU消耗和运行时间的比较。PSMNet、GwcNet和GANet11的GPU显存分别下降了39.97%、36.99%和24.11%。



------

## 5 结论

本文提出了一种用于高分辨率多视点立体视觉和立体匹配的GPU消耗低、计算高效的级联代价体表达方法。我们将单个代价体分解为多个阶段的级联公式。接着，我们利用前一个阶段的深度(或视差)图来缩小每个阶段的深度(或视差)范围，并减少假设平面的总数。接着，我们利用更高空间分辨率的代价体生成更精细的输出。所提出的代价体是对现有的基于3D代价体的多视点立体视觉和立体匹配方法的补充。



------

## 6 附录

### 6.1 讨论

A. 为什么假设范围显著减小？

如图9中我们提供绝对深度误差的统计表示，代表着预测的深度和真值的距离。由于第一阶段没有进行深度预测，我们将真值深度作为第一阶段的绝对深度误差。在图9（a）中，第一级的整个深度范围约为500mm，第二级的整个深度范围约为500mm；图9（b）展示第三阶段缩小到50mm左右，比第一级减少90%。因此，我们可以在第二和第三阶段显著减小假设范围。

> 图9 绝对误差在不同阶段的分布。由于第一阶段没有预测深度，我们假设该阶段的绝对误差为真值深度。统计结果在DTU评价数据集上使用MVSNet+Ours进行三阶段成本量计算。

B. 如何在不同的阶段设置假设范围？

如图10，我们计算绝对误差小于某一阈值的百分比(称为内嵌百分比)。假设范围应该包括大多数错误预测的深度(或视差)并对其进行修正。如图10（a）所示，MVSNet和MVSNet+Ours在第一阶段的内嵌百分比相交于5.92mm和86%。这意味着如果我们设置一个大于5.92mm的假设范围，我们可以覆盖比MVSNet更可能正确的预测，因为我们的级联代价体能够在后期纠正错误的预测。在多视点立体数据集上，我们将假设范围设为$32 \times 2 \times 2.5 = 160mm$，但仍有较大的缩减余地。

同样，立体匹配如图10（b）所示，视差假设范围设置为$12 \times 2 = 24$像素(交集为19.60像素)，与原始的单代价体积方法相比，覆盖了第一阶段更多的错误预测范围。

> 图10 预测与真值之间的绝对误差小于某一阈值的百分比。我们展示了MVSNet，GwcNet的结果，以及在不同阶段具有级联代价体的某些网络。

C. 为什么级联代价体有高效的显存使用？

在多视点立体视觉中，由于整个深度范围较第一阶段缩小了近90% (500mm vs. 50mm)，所以假设范围可以显著减小。因此，我们可以在后期使用较少的假设平面作为代价体。在MVSNet+Ours中，我们在第一阶段设置了48个假设平面，而MVSNet有192个平面，使得GPU显存从10823MB减少到2373MB。为了提高后续阶段的精度，我们提高了空间分辨率，GPU显存从2373MB增加到4093MB和5345MB。虽然我们提高了空间分辨率，但总的GPU显存比MVSNet减少了约50.6%，运行时间提升大约2倍，如图1所示。同样地，在立体匹配中，我们也将GPU显存从3827MB减少到2699MB。

此外，我们可以通过采用不同的级联数、假设范围和空间分辨率来平衡时间(或显存消耗)的效率和准确性。

### 6.2 多视点立体视觉

在本节中，我们将展示更多的多视点立体视觉实验结果。如图14所示，我们在DTU数据集上可视化了MVSNet+Ours重建的点云。

### 6.3 立体匹配

A. Scene Flow数据集上的定性结果

在本节中，我们展示了PSMNet，GwcNet，GANet11和扩展模型PSMNet+Ours, GwcNet+Ours, GANet11+Ours在Scene Flow数据集上的几个重建结果。如图12所示，替换了本文的级联代价体后，感官质量得到提升。

B. 立体匹配中的级联阶段数

在本实验中，我们将GwcNet中的代价体替换为我们提出的级联代价体，即GwcNet+Ours。需要注意的是，GwcNet中的级联量设置为64通道，不同阶段的空间分辨率与原始GwcNet相同。具有$i$个阶段的扩展模型被记为GwcNet-Cas$_i$，如表8中所示，扩展模型的精度随阶段的增加而提高。如图11所示，随着阶段的增加，细节变得更清晰了。

> 表8 GwcNet与使用不同假设平面和深度间隔设置的融合级联代价体的GwcNet方法对比。统计结果在Scene Flow测试数据集上采集。
>
> 表9 级联代价体不同阶段的统计结果。统计结果使用GwcNet+Ours在Scene Flow测试数据集上采集。运行时是当前阶段和之前阶段的总和，原始输入大小是$960 \times 512$。

C. 立体匹配中的空间分辨率

我们研究了立体匹配中代价体的空间分辨率对重建精度和GPU消耗的影响。与多视点立体视觉实验相似，我们基于GwcNet构建了一个三阶段代价体，空间分辨率从原始输入图像的$1/4 \times 1/4$逐渐增大到1。在从粗到细的方法中，端点误差由0.972提高到0.619。相应地，GPU消耗从1545MB增加到3429MB。

### 6.4 限制和未来的工作

本文提出的级联代价体表达从将单个代价体分解为多个阶段的级联表达中提升。我们已经在地6.1节分析了假设范围设置的高效性。虽然级联表达是对现有的基于代价体的多视点立体和视觉立体匹配方法的补充，但仍存在一些局限性。如图13所示，GwcNet+Ours产生的结果是有偏差的，因为早期阶段输出错误的视差，下一阶段的假设范围不能覆盖其相应的真值。需要指出的是，这种情况发生的概率很小，因为根据6.1节的分析，级联代价体表达可以纠正几乎错误的预测，总体性能也优于单一代价体模型。

目前，每个像素的假设范围是相同的。未来的研究工作包括通过整合语义信息确定每个区域的假设范围，但可能需要更灵活的成本量计算方法。



> 图11 GwcNet+Ours每个中间阶段在Scene Flow测试数据集上的重建结果。从左到右：参考图像和真值, stage1、stage2、stage3的预测视差。中间重建的放大区域如下图所示。
>
> 图12 Scene Flow数据集测试集的定性结果。我们展示了几个代表性的立体视觉神经网络和融合级联代价体的扩展模型结果。
>
> 图13 GwcNet+Ours在Scene Flow数据集测试集上的一个失败案例。第一行：参考图像、GwcNe预测结果和GwcNet的误差图；第二行：Ground Truth、GwcNet+Ours的预测结果和GwcNet+Ours的误差图。红色箭头指出了错误的预测区域。
>
> 图14 MVSNet+Ours在DTU评价数据集上的点云结果