# 第二章 理论基础

## 2.1 三维重建基础知识

​		本部分将介绍立体匹配、深度估计、三维重建领域基础的背景知识，主要介绍立体视觉建模中通过对极几何进行表示的方法，多视点图像间的单应变换及投影问题，平面扫描方法进行系列校准后图像中像素深度估计，以及双目视觉三维重建和多目视觉三维重建的基础问题立体匹配。

### 2.1.1 对极几何

​		对极几何（Epipolar Geometry）是对立体视觉建模的一种约束，通过此建模方法使得立体匹配或深度估计问题存在一个最优解。对极几何的示意图如2.1所示，$X$代表三维空间中的某研究点，$C, C'$表示两摄影机中心，$A,A'$代表两成像平面，$x, x'$分别表示$X, X'$在相机成像平面内的投影。图中两摄像机光心的连线$CC'$称为基线，包含基线的平面$CXC'$称为该观察对象的对极平面，一副视图中另一个摄像机中心的像，即基线与两成像平面的交点称为对极点，图中以$e,e'$所示，而对极平面与成像平面的交点$xe, x'e'$称为对极线。

> 2.1
>
> 对极几何示意图

​		需要注意的是，对极几何描述的是两视图间的内在射影关系，只与相机内参和两视图间的相对姿态有关，不依赖外部场景。因此，如果世界中的某研究对象$X$在成像平面$A$上的像点$x$已知，则$X$在另一个构成对极平面的成像平面$A'$上的像点必然在极线$x'e'$上，通过上述极限约束可以大大缩小特征点匹配时的搜索范围。通过对极几何的建模，使得对应点匹配从整幅图像上的寻找压缩到在一条直线上的寻找。

### 2.1.2 单应变换

​		单应变换（Homography）是将空间中的点变换到另一个空间中的投影方式，广泛地用于图像变换中。由单应变换衍生出的单应变换矩阵的一般形式可以表示为
$$
H=\left[\begin{array}{lll}
h_{11} & h_{12} & h_{13} \\
h_{21} & h_{22} & h_{23} \\
h_{31} & h_{32} & h_{33}
\end{array}\right]
$$
​		在图2.1中，假设相机$C$和$C'$间存在旋转$R$和位移$t$，则在对应成像平面上的像点存在关系：$x' = Rx + t$。由此，单应变换矩阵$H$可以通过公式2进行表达，其中$d$可理解为相机$C$距Q的垂直高度，而$n$代表该平面的法向量。
$$
H = K(R + \frac{1}{d}tn^T)K^{-1}
$$
​		在代数中，假设单应变换矩阵$H$将一张图像上的点$a=(x,y,1)$映射到另一张图像上的坐标点$b=(x',y',1)$，根据单应变换的性质和特点可知$b = H a^T$，将其展开并进行变换可得公式2.3。
$$
\left[\begin{array}{ccccccccc}-x & -y & -1 & 0 & 0 & 0 & x x' & y x' & x' \\ 0 & 0 & 0 & -x & -y & -1 & x y' & y y' & y'\end{array}\right]h = 0
$$
​		其中将h为9维列向量$h=\left[h_{11}, h_{12}, h_{13}, h_{21}, h_{22}, h_{23}, h_{31}, h_{32}, h_{33}\right]^{T}$，如令
$$
A = \left[\begin{array}{ccccccccc}-x & -y & -1 & 0 & 0 & 0 & x x' & y x' & x' \\ 0 & 0 & 0 & -x & -y & -1 & x y' & y y' & y'\end{array}\right]
$$
​		则上式可简化为
$$
Ah = 0
$$
​		由于单应变换中采用齐次坐标表示平面上的点，所以存在非零标量$s$，使得$b = sHa^T$与$b' = sHa^T$表示同一点$b$，若令$s = \frac{1}{h_{33}}$，则单应变换矩阵的最后一项系数为1，自由度（Degree of Freedom，简称dof）为8。因此只需两幅图像中的4个点对，就可以求解线性方程组的方式解得两图像间的单应变换关系。如果对应点对多于4对，可以通过最小二乘法求解单应变换矩阵，或通过将矩阵$A$进行SVD分解，即$A = U \times \Sigma \times V^T$，取$V$的最后一列作为对应点求解h。

### 2.1.3 平面扫描

​		平面扫描（Planning Sweeping）方法通过对多台相机对场景中物体拍摄得到的经过校准后的图像进行建模，通过单应变换和构建损失函数求的平面中点在真实物理空间的深度。平面扫描方法基于所有物体只有漫反射的假设，且不考虑场景中的关照变换，构建近平面、远平面和虚拟相机，并将物体用一系列等间距的密集平面进行划分。平面扫描的示意图如图2.2所示。

> 图2.2
>
> 平面扫描示意图

​		如果平行平面划分的足够精细，则物体表面的任意一点$p$一定在某一平面$D_i$上，则可以看到该点$p$的相机记录的图像像素值必定相等，假设与$p$在同一平面的另一点$p'$并不位于物体表面，则投影到各个相机上的像素值不同。因此平面扫描算法假设：对于平面上的任意点$p$，如果其投影到每个相机上的像素值均相同，则该点很大概率上是物体表面的点。

​		平面扫描算法主要包括两个核心步骤：

（1）将平行平面$Di$上的每个点投影到所有相机上，根据投影得到的像素值进行匹配计算，得分越高，则表示投影在各个相机上的像素越接近，此估计的深度值越精准

（2）将得分高的点再投影到虚拟相机$x$上，并从后到前扫描平面，如果某一平面$D_j$上的点$q$投影到虚拟相机上的得分高于之前的分数，则更新该点的深度和分数，直至平面扫描结束

​		在实际问题中，由于光照和噪声等影响，往往不仅使用像素值，还需要结合窗口信息进行比较，并使用损失函数进行优化，公式如下所示，其中$W$代表以$x,y$为中心的窗口，$\beta$为增益因子。
$$
\begin{aligned}
C\left(x, y, \Pi_{k}\right) &=\sum_{k=0}^{N-1} \sum_{(i, j) \in W} \mid I_{r e f}(x-i, y-j)-\beta_{k}^{r e f} I_{k}\left(x_{k}-i, y_{k}-j\right) \mid
\end{aligned}
$$

​		根据损失函数可以通过公式2.6计算各个点在系列平面的最小值，并使用公式2.7计算该点在物理空间的深度
$$
\tilde{\Pi}(x, y)=\underset{\Pi_{m}}{\operatorname{argmin}} C\left(x, y, \Pi_{m}\right)
$$

$$
Z_{m}(x, y)=\frac{-d_{m}}{\left[\begin{array}{cc}
x & y & 1
\end{array}\right] K_{r e f}^{-T} n_{m}}
$$

### 2.1.4 立体匹配




---

## 2.2 深度学习方法基础知识

​		本部分将介绍笔者选用的无监督深度学习方法的理论基础，着重介绍卷积神经网络、无监督深度学习以及用于实现本文算法的深度学习框架Tensorflow，本部分只针对在算法定义与实现中的重点知识介绍，并不涉及相关内容的详细展开。

### 2.2.1 卷积神经网络

​		卷积神经网络（Convolutional Neural Network，简称CNN）是一种模拟生物大脑结构和功能的具有深层次结构的前馈神经网络，是神经网络中的代表网络之一，也是深度学习领域最知名的算法之一。卷积神经网络通过模仿生物神经网络和视感知机制构建，可以用于进行有监督学习和无监督学习。

​		卷积神经网络有着丰富的网络结构，但核心的主要有三层：卷积层（Convolutional Layer）、池化层（Max Pooling Layer）、全连接层（Fully Connected Layer）。

​		A. 卷积层

​		主要用于对输入数据进行特征提取，通过具有不同偏差量和权重系数的的卷积核（convolution kernal）对数据进行卷积操作，进而提取具有某些属性或规律的特征。常见的卷积核大小为一般为3*3，5*5，7*7等，不同的卷积核尺寸影响提取特征的感受野（receptive field ），理论上卷积核越大，感受野越大，提取出的特征越被视为全局特征。除此之外卷积层还有如填充（padding）、步幅（stride）等概念在此不做展开。

​		B. 池化层

​		主要对卷积层提取出的特征进行下采样（down-sampling），不仅降低了数据尺寸加快网络的训练和防止过拟合，同时对特征进行过滤和选择，保留最显著的特征，并不会对数据丢失产生影响。常用的池化操作有最大池化（Max Pooling）、平均池化（Mean Pooling）等。

​		C. 全连接层

​		使用卷积层和池化层提取出的特征进行分类，并通过调整神经网络中参数的权重让网络具有更好的分类效果，即通过得到的高阶特征完成学习任务。

### 2.2.2 无监督深度学习

​		卷积神经网络可以用于实现有监督学习和无监督学习，但随着有监督学习的逐步发展，人们发现优秀的方法不仅受限于算法本身，很大程度上受限于数据。原始的数据获取方法通过研究人员手动构建特征，并通过众包的方式获取手工标注数据，虽然也有AMT（Amazon Mechanical Turk）[3]等工具帮助获取更大的训练数据集，但相较于大量不易标注或无法标注的数据来说仍然是巨大的损失。

​		无监督学习算法建立在“尽管单一未标注样本蕴含的信息比同样标注过的样本少，但如果获取大量的无标注数据，并能找到方法进行充分利用，算法将获得更好的可扩展性和泛化能力”思想上。接下来笔者将依次介绍两类主要的无监督学习方法：确定型的自编码方法和概率型的受限玻尔兹曼机。

​	A. 自编码

​		自编码是一种特殊的三层反向传播（Back Propagation，简称BP）神经网络逆行。不同点在于需要尽可能满足编码无损，即满足自编码网络的输入和输出尽可能的相似。
一个典型的自编码网络结构如图2.1所示。

> 图2.1
>
> 自编码网络结构

​		无监督学习使用的无真值数据$X$首先经过可见层到隐含层的第一次变换$H = Wx + b$，该变换相当于一个编码过程（encoder），此时输入的数据被进行了一定程度的抽象。得到的抽象数据表示再经过隐含层到输出层的转换$\hat X = W'H + b'$，该变换相当于一个解码过程（decoder）。如果输入数据和输出数据在某种度量上相等或相似，则代表该自编码是无损的，我们可以使用公式2.1表示该自编码结构的损失函数。

$$
\frac{1}{2} \times \lVert \hat X - X \rVert ^2
$$

​		但在实际使用自编码结构时，要考虑参数的正则化，此时损失函数如公式2.2所示

$$
\frac{1}{2} \times \lVert \hat x - x \rVert ^2 + \frac{\alpha}{2} \times (\lVert w \rVert ^2 + \lVert w' \rVert ^2)
$$

​		如上所示，我们就可以据此逐步将输入的无真值数据进行转换，并构建损失函数用于无监督学习的训练过程。但需要注意的是，恒等函数始终满足该编码-解码条件，为避免这种情况，一般要求隐含层单元数小于输入输出单元数，例如自编码的输入输出分别为64*64尺寸的图片，则输入单元和输出单元分别为4096个，而隐含层单元可能为100个。

​		随着科学家的发现，生物神经网络中大部分神经元在同一时刻是处于抑制状态的，只有核心的少量神经元被激活。根据这层启发，自编码网络一般还要满足稀疏的特征，即当神经元的输出通过激活函数，如Sigmoid函数后，如果接近1，则认为该神经元被激活，反之如果接近0，则认为该神经元被抑制，可用公式2.3作为隐层神经元$j$在训练中的平均兴奋程度的度量公式
$$
\tilde{\rho_j} = \frac{1}{m} \times \sum^m_{i=1}(f(h_j) x_i)
$$
​		综上，考虑稀疏性的自编码网络损失函数可以表示为公式2.4，其中“距离”使用相对熵（Kullback-Leibler，简称KL）进行度量。根据损失函数，便可以按照BP算法或其他算法最优化自编码网络中的参数。
$$
\frac{1}{2} \times \lVert \hat x - x \rVert ^2 + \frac{\alpha}{2} \times (\lVert w \rVert ^2 + \lVert w' \rVert ^2) + \sum^m_{j=1}(\rho \times log \frac{\rho}{\tilde{\rho_j}} + (1-\rho) \times log \frac{1 - \rho}{1 - \tilde{\rho_j}})
$$



​	B. 受限玻尔兹曼机

​		另一种无监督学习方法是基于概率的方法，而其中最具代表性的就是受限玻尔兹曼机。受限玻尔兹曼机（Restricted Boltzmann Machines，简称RBM）是一类由两层结构组成，且对称连接无反馈的随机神经网络模型。其简要结构如图2.2所示，层间保持全连接，但层内无连接。

> 图2.2
>
> 受限玻尔兹曼机

​		可见层$V$用于观测和表示数据，隐层$h$用于提取特征，$W$为两层间的连接权重，在受限玻尔兹曼机中可见层和隐层可以为任意的指数族单元，如softmax单元、泊松单元、高斯单元等。假设所有可见单元和隐单元均为二值变量，即$\forall i, j, v_i \in \{0, 1\}, h_j \in \{0,1\}$，对于一组给定的可见层和隐层单元状态$(v,h)$，和一组参数参数$\theta = \{W_{ij}, a_i, b_j \}$，受限玻尔兹曼机系统的能量如公式2.5所示
$$
E(v, h \mid \theta)=-\sum_{i=1}^{n} a_{i} v_{i}-\sum_{j=1}^{m} b_{j} h_{j}-\sum_{i=1}^{n} \sum_{j=1}^{m} v_{i} W_{i j} h_{j}
$$
​		基于系统的能量函数，对于一组给定的可见层和隐层单元状态$(v,h)$的联合概率密度分布表示如公式2.6所示，其中$Z(\theta)$为归一化因子的能量和，即所有可能情况下的能量总和。
$$
P(v, h \mid \theta)=\frac{e^{-E(v, h \mid \theta)}}{Z(\theta)}, \quad Z(\theta)=\sum_{v, h} e^{-E(v, h \mid \theta)}
$$
​		当使用训练好参数权重的受限玻尔兹曼机进行观测时，如数据$v$的分布$P(v \lvert \theta)$，通过对公式2.6求解边缘分布即可，如公式2.7所示
$$
P(v \lvert \theta) = \frac{1}{Z(\theta)}\sum_h e^{-E(v, h \lvert \theta)}
$$
​		但此公式需要计算$Z(\theta)$，需要对所有的可见层单元和隐层单元累加，计算量很大。但由受限玻尔兹曼机的特殊结构可知，当给定可见层单元的状态时，各隐层单元的激活状态间条件独立，此时，第$j$个隐层单元的激活概率为
$$
P(h_j = 1 \lvert v, \theta) = \sigma(b_j + \sum_i v_i W_{ij})
$$
​		其中$\sigma$为sigmod激活函数，具体表示为公式2.9
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$
​		根据受限玻尔兹曼机的结构对称性，在给定隐层单元状态时，各可见层单元的激活状态也是条件独立的，此时，第$i$

个可见单元的激活概率为
$$
P(v_i = 1 \lvert v, \theta) = \sigma(a_i + \sum_j W_{ij} h_j)
$$
​		而受限玻尔兹曼机神经网络的求解任务$\theta = \{W_{ij}, a_i, b_j \}$只需通过最大化无真值训练集上的对数释然函数即可进行训练，由公式2.11表示
$$
\theta^{*}=\underset{\theta}{\arg \max } \mathcal{L}(\theta)=\underset{\theta}{\arg \max } \sum_{t=1}^{T} \log P\left(v^{(t)} \mid \theta\right)
$$


​	

-----

```
[1]周飞燕,金林鹏,董军.卷积神经网络研究综述[J].计算机学报,2017,40(06):1229-1251.

[2]殷瑞刚,魏帅,李晗,于洪.深度学习中的无监督学习方法综述[J].计算机系统应用,2016,25(08):1-7.

[3]Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing: Quality Management on Amazon Mechanical Turk, 2010.

[4]Geoffrey E. Hinton; Training Products of Experts by Minimizing Contrastive Divergence. Neural Comput 2002; 14 (8): 1771–1800. 
```

