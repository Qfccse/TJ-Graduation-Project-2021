# Learning Unsupervised Multi-View Stereopsis via Robust Photometric Consistency

## 关键词
鲁棒广度一致性；无监督深度学习

## 摘要
现在的基于深度学习的MVS算法强烈依赖3D场景的真值作为训练数据，但是获取如此精确的3D几何信息是主要障碍
利用多个视角间的光度一致性作为监督信号
简单使用图像之间的一致性会受occlusion和不同视角间光度变换的影响

我们提出鲁棒的损失函数
1. 强制一阶一致性
2. 对于每个点，选择性地加强与某些视图的一致性，从而隐式地处理遮挡

定性观察我们的重建效果甚至比获取到的真值更加完整
该模型可以推广到新的设置，允许现有的CNN通过无监督微调适应没有真值的3D数据集

## 1. 背景介绍
当前的MVS方法主要是利用潜在的几何一致和光度一致 —— 在纵深方向将图像中的每一点投影到参考图像，正确的匹配是光度一致的。这些纯粹的基于几何的方法能够独立分析每个场景，并不能隐含地捕捉和利用关于世界的一般先验知识，例如：表面往往是平的，因此当稀疏时表现很差；表面材质确实情况表现很差

为克服这些局限，一条思路专注于基于深度学习的方法，典型的是训练CNN网络跨视图提取和合并信息，他们很有效，但在训练过程中强烈依赖于场景的真值，这种形式的监督过于繁重且真值很难获取（尤其是土木场景….），寻找不依赖于这种监督的解决方案具有实际和科学效益。

我们以这些最近的基于深度学习的MVS方法为基础，但在监督形式上不需要依赖3D场景的真值，而只需要多视点图像集，正确的估计将产生光度一致的重投影，因此我们可以通过最小化重投影误差以训练CNN网络

相似的重投影误差已经在单目深度估计上比较好的应用，但是如果直接移植到多目深度估计并不完善，因为不同的图像捕捉到的是不同可见程度的场景

某一个特定的点不需要在所有其他视图中都保持光度一致，而只需要在没被遮挡的地方计算重投影误差。同时不同视角的光照变化也对结果产生极大的影响，因此，只在像素空间强制一致性是不合适的，我们的方法是额外强制基于梯度的一致性。我们提出了鲁棒的重投影损失解决这些问题，并且允许使用无监督的方式进行学习。

我们简单、直观的公式高效的处理遮挡，而无需明确的建模它们

> 参考这个在论文里放图
> [image:A1AAE4B6-35D9-4360-8BE6-7D40FE599D7B-23459-00010756FA93E094/DB41D580-A0F9-4FEE-B808-E114870862A2.png]


【主要贡献】
- 基于无监督深度学习的MVS框架，只需要使用输入图像的信息作为监督信号
- 鲁棒的无监督光度一致性损失函数进行无监督深度预测，并克服光照变化和训练视图遮挡的问题

---

## 相关工作
【多视点三维重建】
MVS pipeline的四个主要步骤
1. 场景选择
2. 传播方案
3. 补丁匹配（查一查术语？：基于patch match[5]的场景匹配取代了传统的seed-and-expand，结合迭代传播方案，估计深度和法线
4. 深度图融合：融合多张深度图为完整的点云，同时确保融合后的点云具有相当的一致性且错误估计的离群值被剔除

~深度图估计也同时将多视点三维重建问题进行解藕~

【基于深度学习的MVS】
CNN提取的图像特征和学习指标被广泛的用于MVS问题中
- 通过体素表示编码不同视点的表面可见性[19,21]，难以扩展到更多样化和大规模的场景中
- [22] 使用CNN特征和视差构建的代价体通过可微的softmax进行回归
- [36,15,34] 使用基于有监督的深度学习方法通过多视点进行三维重建

【无监督深度估计】
[9,7,26,42]基于无监督深度学习单目或双目立体匹配防范利用光度一致性作为损失函数进行训练，但是他们的训练大多依赖于帧之间的视觉变换信息，因此很难泛化到遮挡和光照变化的场景。

---
## 3. 方法原理
输入：一组图片（中心/参考图像，相邻视角图像） + 相机内参和外参（可以通过预先增加Structure-from-Motion，Sfm步骤获取）  与之前的深度学习方法不同不需要3D场景的真值
输出：对于每一张输入估计像素级的深度图，再通过后处理将深度图融合成点云

利用光度一致性误差训练深度预测CNN网络，在原始图像和新图像差异的部分进行惩罚。然而，光度一致不是一直成立的，某一个点在某些视角下可能被遮挡而不可见，同时不同视角的光照变换可能会导致像素颜色（强度）之间的进一步差异

::为了考虑可能的光照变换，在光度损失中添加了一阶一致性项；同时在梯度匹配中也要保证颜色（强度）匹配::
::然后，隐式地处理可能的遮挡，提出一个稳健的光度损失：某一个点需要与某些点满足一致性，但不一定必须与所有点一致::

### 网络结构
[36, 15, 34]作为基础
1. 输入N张图像
2. 通过CNN提取图像特征
3. 使用可微投影变换和平面扫描构建代价体（将图像特征按照不同深度进行投影）
4. 对每个参考图像预测深度，输出缩小分辨率的深度图

核心在于使用鲁棒的光照损失作为无监督指标

### 传统原始版本
核心是使用基于投影的视图合成损失，该方法在单目和双目深度估计中取得了相当的效果[43,27]，但是在无监督多目立体匹配中很少被使用

给定输入图像Is和附加的相邻帧视图，还需要使用M个附加的新视角用来监督生成预测深度图Ds
[image:A1A995E8-2A1E-4C48-B46F-26A6A5D93628-23459-00010DFCD1C82707/2118CC43-1FD8-4275-AAB1-9761DF731273.png]
对于一对具有相关相机参数（K，T）的像素点对（Is，Ivm），通过预测得到的Ds深度图，实现逆投影，可以使用空间转换网络进行可微的双线性抽样[16] 将新视点投影得到Isi_hat
[image:CD1B7A8D-C76F-494D-97C8-62C30F9ACEF8-23459-00010E5444ABAC14/AD17A41A-9142-4EDC-ABEE-8D8F725B66C3.png]

对于原图Is中的某一个像素u，可以在新视图中通过投影得到新的值：
[image:B464A4E8-6448-4CB1-BEAF-D67124C93477-23459-00010E6D4D7C08D3/ECD6FCF0-B1E6-4891-AA61-C6E23B81348C.png]

通过对投影坐标周围的新视图中进行双线性采样，得到投影后的图像
[image:F5544CB8-5029-4F7F-8FAE-8CB018D14834-23459-00010EB119792501/4F267573-5900-4E25-82AA-7DD0393E5171.png]

同时生成了一个二元验证mask Vsm，表示合成视图中的“有效”像素，因为一些像素投影到新视图中会位于图像边界之外，然后我们可以制定一个图像一致性的目标，制定投影突袭那个和源图像的匹配关系。在多视点中就可以自然的扩展到所有M张新图片和参考图像间的逆投影

loss为
[image:5EC121DF-20D5-4074-ABCA-3B44A8896B3F-23459-00010EE7549805D5/BD107907-EA30-46CE-A1DB-4998E91249B1.png]

【不足】
- 不能很好的处理遮挡和光照变化
- 这些单目和双目的方法主要是在KITTI数据集中进行训练，这些数据集中只有很少的遮挡和光照变换
- 但是MVS数据集中，自遮挡、反射、阴影都会对结果造成很大的影响


### 鲁棒的光度损失版本
【提出新方法基于的观察】
- 相比于颜色（强度/亮度），图像梯度对光照的变化有更好的一致性 more invariant
- 一个点只需要在光度上与一些点（非遮挡的点）保持一致，而非所有新视点中的对应点

【改进一】
- 基于图像绝对强度和图像梯度差异的匹配成本比仅考虑成本效果更好
- 由于图像之间像素变化很大，用huber loss描述图像间的差距很重要
修改基于逆投影的光照损失（公式3） ，得到::一阶一致性损失::
[image:DD64C08C-F95C-45E4-98F9-9678C8801DE1-23459-00010F6287AF973E/489087A5-BE36-4128-9FD5-A71D3E6A359B.png]

【改进二】
由于遮挡，某一点不需要跟所有新视点的投影一致
[image:69953E2D-F529-462E-ABB1-9B6CFC954565-23459-00010F8E9EE535F1/6AF58E49-F662-4BEF-AD02-76138D9013F9.png]

只保证每个像素的与top-K个视点的投影保持一致
::鲁棒的光度损失::
[image:B2BFDB34-B323-45CA-BDD8-6D63DA1B37F2-23459-00010FC93E2FD4F2/E8955091-8B6D-4F83-A304-A866EB2AECF6.png]

对每一个像素u，在像素投影有效的视图中，使用最好的K个不相交的视图计算损失

> 这张图很重要，但还不是看的太懂
> [image:FCA8AC8F-AD41-4183-A443-8566CCA4097F-23459-00011004E31976B4/E2BA5DD6-9A96-43F7-AA4F-644330C3496D.png]
将M个新视点的图像逆投影到参考图像，并逐像素计算一阶一致性的loss-map，所有M个loss-maps集合成3D代价体（W*H*M），对于每个像素，找到K个最小的有效mask，累加它们得到像素级的一致性损失


### Details
输入：一张source，N=2张相邻视点，M=6张新图像novel，K=3
可以从更大的图像集中提取监控信号，而在推理时只需要一个较小的图像集

借鉴单目深度估计中 结构相似度Lssim 和 深度平滑Lsmooth 目标 [27]
- 平滑损失：在训练中加强边缘平滑的激励信号
- SSIM损失：更高阶的重建损失，但由于基于更大的图像补丁patch（在这里没使用），在这里使用的是具有更高视图选择分数的两个相邻视图被用来计算SSIM损失

::最终的学习目标是之前的loss的加权组合::
[image:2E990A0E-D1DF-4666-B0C9-255A21A2F861-23459-000110C87001EED2/B895FFED-786E-4102-8F5A-07B7612B2631.png]
由于GPU性能的影响，在训练中使用稍小的图像尺寸 和 粗depth steps；但在预测中使用更高的设置

---
## 4. 实验
DTU数据集包含124个不同场景，通过机械臂获取的3D结构和高质量RGB图像，对于每个场景有49张图像和高精度相机位姿，使用SurfaceNet和MVSNet相同的训练-验证-测试数据集
与MVSNet相同，对于某一个参考图像，输入的临近图像通过 view-selection score进行选择，使用稀疏点云和相机基线为给定的参考图像选择最合适的近邻视图，我们在训练时也适用相似的M个近邻用作自监督信号，top-K个用来计算损失

【train】
放缩图像尺寸到640*512
N=3，每次用一张参考图像和两张新图像来预测深度图
M=6，K=3，6张临近图像用来计算光度体积损失，对于每个像素选择最好的三个

【test】
dmin dmax = 425mm 935mm

### 结果、定性分析、定量分析
【指标】
- 准确性：预测的深度值与真值的平均距离
- 完整性：真值距离重建的平均距离（这个叙述有点奇怪？
- 综合表现：acc和comp的均值

- 比例指标：overall acc
- f-score：over comp

测试三种组合的表现
- 公式3：原始光度图像损失 + SSIM + Smooth
- 公式4：光度一致 + 一阶梯度一致性损失：对光照更加鲁棒
- 光度 + 一阶 + top-K

【优点】
- 虽然一些经典方法在非常低的阈值上比我们更准确，但是我们产生更少的离群值
- 在提高分辨率方面优于所有其他方法（完全监督的MVSNet除外）
- 更高质量重建效果
- 低纹理区域性能更好
- 平滑的重建，有更少的洞或缺失区域


### 消融实验
对鲁棒损失函数的各个部分影响进行

【top-K】
不同排序的图像的top-K个中被选为作为有贡献的图像（没遮挡）的频率
1. 视图的选择频率与视图选择得分成正比（挑选用于训练的图像集的选择标准直接影响光照一致性）
2. 超过50%的选择来自排名低于2的视图（从附加图片中进行投影验证会带来更好的性能）

【top-K选择阈值】
通过实验，50%不选择的图像有更好的验证效果
[Table 3]

【loss项】
通过绝对误差 / 1mm之内的误差 / 3mm之内的分别进行试验
[Table 2]


### 可扩展性
不使用真值情况下，微调一致性损失，使预训练模型适应数据集
> 在Eth3D数据集上的简单测试（没详细看）

### 泛化测试
我们的方法训练匹配图像的对应关系，而不是记忆场景的先验，应该能更好的概括看不到区域的数据
缩放图像到832*512，使用256深度间隔的平面扫描体
在开放世界的场景中，场景深度范围非常大，并不完全适用于当前的MVS深度架构，因为它们都依赖于某种体积公式
未来的方向可能是以更高的分辨率对深度进行采样以获得更高质量的重建，目前在计算上开销太大


---
## 5. 结论和讨论
【总体的两大点贡献】
- 无监督深度学习框架
- 提出鲁棒的光学一致性损失函数

### 5.2 模型架构
与MVSNet保持一致，核心改造在 没有真值情况下的损失表达

所有输入图像首先经过特征提取网络，所有网络的权值在所有图像中共享；使用一个8层的CNN，在每一次卷积操作后使用批处理归一化和ReLU层，直到倒数第二层；最后一层对于每张图像输出32维度下采样得到的特征图
利用可微单应公式，将特征图按128个深度值投影到参考相机不同的正平行平面中，对于每个非参考图像生成一个代价体
使用基于方差的成本度量，将所有代价体聚集成一个单一的量
为了降低GPU消耗，将MVSNet中的256个深度值精简到128个

使用三层的3D UNet细化代价体以及平滑深度值变化，原始估计的深度图可以通过沿深度通道执行argmin操作获得；相比于赢家通吃策略中的不可微方法argmax操作，这种代价体的聚集方法可以达到亚像素精度，同时由于其可微性，更利于后续的训练；因此，尽管构建代价体的深度值是离散的，得到的深度图遵循连续分布

输出的代价体代表的结果分布很可能由于离群值而不止包含单峰，通过建立深度估计质量，即四个最近的深度估计值的概率累计。然后，估计值经过0.8的阈值滤波，并作为输出代价体的mask

然后将预测的深度图与参考图像一同通过四层的CNN，得到深度残差图。将得到的残差图与初始估计的深度图加权融合得到最终的深度图

### 损失机制
主要目标是最大限度减少由视图合成造成的光度一致性，同时我们增加两个额外的成分来改善模型性能
- 图像patch级别的结构化相似性损失
- 深度图梯度图像感知平滑损失

【SSIM】
使用结构相似性作为损失项
[image:0EFDEA1A-DA59-4CE5-8608-DD6A3CFE6E71-23459-0001174AA38B7D65/2413D431-F5FD-471A-AAF8-31CEA5410D14.png]
miu 局部平均  sigma 方差
SSIM为1代表最高的相似度

对于图像对的SSIM损失为
[image:D8070814-DF33-460C-9005-7D56711151CC-23459-0001176F37BEEE1C/D0CBFABB-C1AF-49D5-B913-3BD4F37674C8.png]
Msij排除了图像扭曲后投影在源图像之外的所有像素，忽略这些区域可以改善边界附近的深度预测
我们只在参考图像和根据视图选择评分排序中最近的两个图像之间应用SSIM损失

【深度平滑损失】
用以激励平滑梯度变换，根据[27] 在深度梯度上添加了一个l1惩罚
[image:8BE4B4FD-47C1-496A-9EEC-6F2DA9905DA6-23459-000117B5502E4C5B/DAEEE1E6-F9F3-4804-9BB7-0B8A698D06D8.png]
